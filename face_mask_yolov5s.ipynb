{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gb7RlfW-JWoU",
    "tags": []
   },
   "source": [
    "# Get Data From Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RuC-7gOtz2Si"
   },
   "outputs": [],
   "source": [
    "!pip install opendatasets\n",
    "!pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nxS6wdVo59ZC"
   },
   "outputs": [],
   "source": [
    "import opendatasets as od\n",
    "\n",
    "\n",
    "od.download(\"https://www.kaggle.com/datasets/andrewmvd/face-mask-detection\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1d7isOkPJiHh"
   },
   "source": [
    "# Convert Annotation File Format From 'XML' To 'TXT' For Yolo v5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "executionInfo": {
     "elapsed": 409,
     "status": "ok",
     "timestamp": 1675971070553,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "hvTNfJwqApME"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from IPython.display import Image  # for displaying images\n",
    "import os \n",
    "import random\n",
    "import shutil\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "from torch.utils.data import Dataset, random_split\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from PIL import Image, ImageDraw\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "random.seed(108)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 414,
     "status": "ok",
     "timestamp": 1675960181007,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "34sdiL2LAkhd"
   },
   "outputs": [],
   "source": [
    "# Function to get the data from XML Annotation\n",
    "def extract_info_from_xml(xml_file):\n",
    "    root = ET.parse(xml_file).getroot()\n",
    "    \n",
    "    # Initialise the info dict \n",
    "    info_dict = {}\n",
    "    info_dict['bboxes'] = []\n",
    "\n",
    "    # Parse the XML Tree\n",
    "    for elem in root:\n",
    "        # Get the file name \n",
    "        if elem.tag == \"filename\":\n",
    "            info_dict['filename'] = elem.text\n",
    "            \n",
    "        # Get the image size\n",
    "        elif elem.tag == \"size\":\n",
    "            image_size = []\n",
    "            for subelem in elem:\n",
    "                image_size.append(int(subelem.text))\n",
    "            \n",
    "            info_dict['image_size'] = tuple(image_size)\n",
    "        \n",
    "        # Get details of the bounding box \n",
    "        elif elem.tag == \"object\":\n",
    "            bbox = {}\n",
    "            for subelem in elem:\n",
    "                if subelem.tag == \"name\":\n",
    "                    bbox[\"class\"] = subelem.text\n",
    "                    \n",
    "                elif subelem.tag == \"bndbox\":\n",
    "                    for subsubelem in subelem:\n",
    "                        bbox[subsubelem.tag] = int(subsubelem.text)            \n",
    "            info_dict['bboxes'].append(bbox)\n",
    "    \n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "executionInfo": {
     "elapsed": 426,
     "status": "ok",
     "timestamp": 1675962020715,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "aT36EPQTGOIr"
   },
   "outputs": [],
   "source": [
    "def convert_to_yolov5(info_dict, base_dir):\n",
    "    print_buffer = []\n",
    "    \n",
    "    # For each bounding box\n",
    "    for b in info_dict[\"bboxes\"]:\n",
    "        try:\n",
    "            class_id = class_name_to_id_mapping[b[\"class\"]]\n",
    "        except KeyError:\n",
    "            print(\"Invalid Class. Must be one from \", class_name_to_id_mapping.keys())\n",
    "        \n",
    "        # Transform the bbox co-ordinates as per the format required by YOLO v5\n",
    "        b_center_x = (b[\"xmin\"] + b[\"xmax\"]) / 2 \n",
    "        b_center_y = (b[\"ymin\"] + b[\"ymax\"]) / 2\n",
    "        b_width    = (b[\"xmax\"] - b[\"xmin\"])\n",
    "        b_height   = (b[\"ymax\"] - b[\"ymin\"])\n",
    "        \n",
    "        # Normalise the co-ordinates by the dimensions of the image\n",
    "        image_w, image_h, image_c = info_dict[\"image_size\"]  \n",
    "        b_center_x /= image_w \n",
    "        b_center_y /= image_h \n",
    "        b_width    /= image_w \n",
    "        b_height   /= image_h \n",
    "        \n",
    "        #Write the bbox details to the file \n",
    "        print_buffer.append(\"{} {:.3f} {:.3f} {:.3f} {:.3f}\".format(class_id, b_center_x, b_center_y, b_width, b_height))\n",
    "        \n",
    "    # Name of the file which we have to save \n",
    "    save_file_name = os.path.join(base_dir, info_dict[\"filename\"].replace(\"png\", \"txt\"))\n",
    "    \n",
    "    # Save the annotation to disk\n",
    "    print(\"\\n\".join(print_buffer), file= open(save_file_name, \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1675971384194,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "ldD5PgReA9Gc"
   },
   "outputs": [],
   "source": [
    "class_name_to_id_mapping = {\"without_mask\": 0,\n",
    "                            \"with_mask\": 1,\n",
    "                            \"mask_weared_incorrect\": 2}\n",
    "\n",
    "annotations_path = \"/content/face-mask-detection/annotations\"\n",
    "annotations_to_path = \"/content/face-mask-detection/yolo_annotations\"\n",
    "images_path = \"/content/face-mask-detection/images\"\n",
    "root_dir = \"/content/face-mask-detection\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "executionInfo": {
     "elapsed": 392,
     "status": "ok",
     "timestamp": 1675962393508,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "YXhQIa9TFI5o"
   },
   "outputs": [],
   "source": [
    "for f in os.listdir(annotations_path):\n",
    "    if f[-3:] == \"xml\":\n",
    "        file_path = os.path.join(annotations_path, f)\n",
    "        xml_data = extract_info_from_xml(file_path)\n",
    "        convert_to_yolov5(xml_data, annotations_to_path)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gs67IFx-MQKU"
   },
   "source": [
    "##Test The Annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "765IlAOCKNO8"
   },
   "outputs": [],
   "source": [
    "annotations = [os.path.join(annotations_to_path, x) for x in os.listdir(annotations_to_path) if x[-3:] == \"txt\"]\n",
    "\n",
    "random.seed(0)\n",
    "\n",
    "class_id_to_name_mapping = dict(zip(class_name_to_id_mapping.values(), class_name_to_id_mapping.keys()))\n",
    "\n",
    "def plot_bounding_box(image, annotation_list):\n",
    "    annotations = np.array(annotation_list)\n",
    "    w, h = image.size\n",
    "    \n",
    "    plotted_image = ImageDraw.Draw(image)\n",
    "\n",
    "    transformed_annotations = np.copy(annotations)\n",
    "    transformed_annotations[:,[1,3]] = annotations[:,[1,3]] * w\n",
    "    transformed_annotations[:,[2,4]] = annotations[:,[2,4]] * h \n",
    "    \n",
    "    transformed_annotations[:,1] = transformed_annotations[:,1] - (transformed_annotations[:,3] / 2)\n",
    "    transformed_annotations[:,2] = transformed_annotations[:,2] - (transformed_annotations[:,4] / 2)\n",
    "    transformed_annotations[:,3] = transformed_annotations[:,1] + transformed_annotations[:,3]\n",
    "    transformed_annotations[:,4] = transformed_annotations[:,2] + transformed_annotations[:,4]\n",
    "    \n",
    "    for ann in transformed_annotations:\n",
    "        obj_cls, x0, y0, x1, y1 = ann\n",
    "        plotted_image.rectangle(((x0,y0), (x1,y1)))\n",
    "        \n",
    "        plotted_image.text((x0, y0 - 10), class_id_to_name_mapping[(int(obj_cls))])\n",
    "    \n",
    "    plt.imshow(np.array(image))\n",
    "    plt.show()\n",
    "\n",
    "# Get any random annotation file \n",
    "annotation_file = random.choice(annotations)\n",
    "with open(annotation_file, \"r\") as file:\n",
    "    annotation_list = file.read().split(\"\\n\")[:-1]\n",
    "    annotation_list = [x.split(\" \") for x in annotation_list]\n",
    "    annotation_list = [[float(y) for y in x ] for x in annotation_list]\n",
    "\n",
    "#Get the corresponding image file\n",
    "image_file = annotation_file.replace(\"yolo_annotations\", \"images\").replace(\"txt\", \"png\")\n",
    "assert os.path.exists(image_file)\n",
    "\n",
    "#Load the image\n",
    "image = Image.open(image_file)\n",
    "\n",
    "#Plot the Bounding Box\n",
    "plot_bounding_box(image, annotation_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RabIljo8P4-s"
   },
   "source": [
    "# Data Split and Make Train/Val/Test Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "executionInfo": {
     "elapsed": 410,
     "status": "ok",
     "timestamp": 1675971526972,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "56lEUpcxf78u"
   },
   "outputs": [],
   "source": [
    "# Read images and annotations\n",
    "images_folder_path = os.path.join(root_dir, 'images')\n",
    "annotation_folder_path = os.path.join(root_dir, \"yolo_annotations\")\n",
    "\n",
    "images = [os.path.join(images_folder_path, x) for x in os.listdir(images_folder_path)]\n",
    "annotations = [os.path.join(annotation_folder_path, x) for x in os.listdir(annotation_folder_path) if x[-3:] == \"txt\"]\n",
    "\n",
    "images.sort()\n",
    "annotations.sort()\n",
    "\n",
    "# Split the dataset into train-valid-test splits \n",
    "train_images, val_images, train_annotations, val_annotations = train_test_split(images, annotations, test_size = 0.2, random_state = 1)\n",
    "val_images, test_images, val_annotations, test_annotations = train_test_split(val_images, val_annotations, test_size = 0.5, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "executionInfo": {
     "elapsed": 646,
     "status": "ok",
     "timestamp": 1675971900182,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "ejlEgmDnseRU"
   },
   "outputs": [],
   "source": [
    "!mkdir -p images/train -p images/val -p images/test -p annotations/train -p annotations/val -p annotations/test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJg2huCCtsBl"
   },
   "source": [
    "**Move Files**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "executionInfo": {
     "elapsed": 438,
     "status": "ok",
     "timestamp": 1675971951977,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "nMvfvtPMtlar"
   },
   "outputs": [],
   "source": [
    "#Utility function to move images \n",
    "def move_files_to_folder(list_of_files, destination_folder):\n",
    "    for f in list_of_files:\n",
    "        try:\n",
    "            shutil.move(f, destination_folder)\n",
    "        except:\n",
    "            print(f)\n",
    "            assert False\n",
    "\n",
    "# Move the splits into their folders\n",
    "move_files_to_folder(train_images, 'images/train')\n",
    "move_files_to_folder(val_images, 'images/val/')\n",
    "move_files_to_folder(test_images, 'images/test/')\n",
    "move_files_to_folder(train_annotations, 'annotations/train/')\n",
    "move_files_to_folder(val_annotations, 'annotations/val/')\n",
    "move_files_to_folder(test_annotations, 'annotations/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LmjtlcF9udGW"
   },
   "source": [
    "#Install YoloV5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JKOqA_8xuOBU"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5\n",
    "!pip install -r yolov5/requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 495,
     "status": "ok",
     "timestamp": 1675972198151,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "UHZFKVEYt5k9",
    "outputId": "7313805c-7d21-4bbc-aa69-7cfa426681f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: line 0: cd: ../yolov5: No such file or directory\n"
     ]
    }
   ],
   "source": [
    "!mv annotations labels\n",
    "!cd ../yolov5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VZTIgDmFpg8g"
   },
   "source": [
    "##Define Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "executionInfo": {
     "elapsed": 567,
     "status": "ok",
     "timestamp": 1675973567201,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "ovfuwZeNpeiw"
   },
   "outputs": [],
   "source": [
    "batch = 20\n",
    "epochs = 10\n",
    "cfg = \"/content/yolov5/models/yolov5m.yaml\"\n",
    "hyp = \"\"\n",
    "name = \"/content/runs/train/facemask\"\n",
    "train_imgs_path = \"../images/train\"\n",
    "val_imgs_path = \"../images/val\"\n",
    "test_imgs_path = \"../images/test\"\n",
    "nc = 3\n",
    "names = [\"without_mask\", \"with_mask\", \"mask_weared_incorrect\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "executionInfo": {
     "elapsed": 513,
     "status": "ok",
     "timestamp": 1675973642328,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "hapKrGmMy0AU"
   },
   "outputs": [],
   "source": [
    "import yaml\n",
    "\n",
    "\n",
    "dict_file = [\n",
    "    {\"path\": \"/content\"},\n",
    "    {\"train\": train_imgs_path},\n",
    "    {\"val\": val_imgs_path},\n",
    "    {\"test\": test_imgs_path},\n",
    "    {\"nc\": nc},\n",
    "    {\"names\": names}\n",
    "]\n",
    "with open(\"/content/yolov5/data/face_mask_detection.yaml\", \"w\") as file:\n",
    "    yaml.dump(dict_file, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1675975801826,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "xp9V9S0K32gs"
   },
   "outputs": [],
   "source": [
    "!pip install clearml\n",
    "!pip install comet_ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RR0iQdSjMwkU"
   },
   "source": [
    "##Train Yolov5s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1311259,
     "status": "ok",
     "timestamp": 1675978987856,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "TTF3uo3D7ivY",
    "outputId": "544baf67-c4b4-441e-8bd3-723b893b2be9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=face_mask_detection.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=3, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, bucket=, cache=ram, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=exp, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m⚠️ YOLOv5 is out of date by 1 commit. Use `git pull` or `git clone https://github.com/ultralytics/yolov5` to update.\n",
      "YOLOv5 🚀 v7.0-89-g35d6d9f Python-3.8.10 torch-1.13.1+cu116 CPU\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "\u001b[34m\u001b[1mClearML: \u001b[0mWARNING ⚠️ ClearML is installed but not configured, skipping ClearML logging. See https://github.com/ultralytics/yolov5/tree/master/utils/loggers/clearml#readme\n",
      "WARNING:utils.loggers.comet:COMET WARNING: Comet credentials have not been set. Comet will default to offline logging. Please set your credentials to enable online logging.\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch, fastai, tensorboard, sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: Using '/content/yolov5/.cometml-runs' path as offline directory. Pass 'offline_directory' parameter into constructor or set the 'COMET_OFFLINE_DIRECTORY' environment variable to manually choose where to store offline experiment archives.\n",
      "Downloading https://ultralytics.com/assets/Arial.ttf to /root/.config/Ultralytics/Arial.ttf...\n",
      "100%|██████████| 755k/755k [00:00<00:00, 24.0MB/s]\n",
      "Downloading https://github.com/ultralytics/yolov5/releases/download/v7.0/yolov5s.pt to yolov5s.pt...\n",
      "100%|██████████| 14.1M/14.1M [00:00<00:00, 23.4MB/s]\n",
      "\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     21576  models.yolo.Detect                      [3, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7027720 parameters, 7027720 gradients, 16.0 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/labels/train... 682 images, 0 backgrounds, 0 corrupt: 100%|██████████| 682/682 [00:07<00:00, 93.00it/s] \n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/labels/train.cache\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.1GB ram):  17%|█▋        | 116/682 [00:01<00:08, 63.24it/s]libpng warning: iCCP: Not recognizing known sRGB profile that has been edited\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mCaching images (0.5GB ram): 100%|██████████| 682/682 [00:11<00:00, 58.53it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/labels/val... 85 images, 0 backgrounds, 0 corrupt: 100%|██████████| 85/85 [00:01<00:00, 64.06it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/labels/val.cache\n",
      "\u001b[34m\u001b[1mval: \u001b[0mCaching images (0.1GB ram): 100%|██████████| 85/85 [00:01<00:00, 54.86it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m5.56 anchors/target, 0.999 Best Possible Recall (BPR). Current anchors are a good fit to dataset ✅\n",
      "Plotting labels to runs/train/exp2/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 2 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/exp2\u001b[0m\n",
      "Starting training for 3 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/2         0G     0.1055    0.06213    0.03396         72        640: 100%|██████████| 43/43 [16:58<00:00, 23.68s/it]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/3 [00:00<?, ?it/s]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  33%|███▎      | 1/3 [00:12<00:25, 12.73s/it]WARNING ⚠️ NMS time limit 2.100s exceeded\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 3/3 [00:39<00:00, 13.09s/it]\n",
      "                   all         85        320      0.392     0.0861     0.0303    0.00632\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/2         0G    0.07779    0.05392    0.02186        108        640: 100%|██████████| 43/43 [16:24<00:00, 22.89s/it]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 3/3 [00:37<00:00, 12.62s/it]\n",
      "                   all         85        320      0.764      0.177      0.124     0.0439\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/2         0G    0.06514    0.04623     0.0209         61        640: 100%|██████████| 43/43 [16:04<00:00, 22.42s/it]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 3/3 [00:36<00:00, 12.16s/it]\n",
      "                   all         85        320       0.76      0.205      0.122     0.0417\n",
      "\n",
      "3 epochs completed in 0.856 hours.\n",
      "Optimizer stripped from runs/train/exp2/weights/last.pt, 14.4MB\n",
      "Optimizer stripped from runs/train/exp2/weights/best.pt, 14.4MB\n",
      "\n",
      "Validating runs/train/exp2/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 3/3 [00:38<00:00, 12.95s/it]\n",
      "                   all         85        320      0.764      0.179      0.124     0.0438\n",
      "          without_mask         85         31          1          0     0.0623     0.0174\n",
      "             with_mask         85        283      0.293      0.537      0.309      0.113\n",
      " mask_weared_incorrect         85          6          1          0     0.0011   0.000461\n",
      "Results saved to \u001b[1mruns/train/exp2\u001b[0m\n",
      "COMET INFO: ----------------------------------\n",
      "COMET INFO: Comet.ml OfflineExperiment Summary\n",
      "COMET INFO: ----------------------------------\n",
      "COMET INFO:   Data:\n",
      "COMET INFO:     display_summary_level : 1\n",
      "COMET INFO:     url                   : [OfflineExperiment will get URL after upload]\n",
      "COMET INFO:   Metrics [count] (min, max):\n",
      "COMET INFO:     metrics/mAP_0.5 [3]      : (0.03026927055055537, 0.12396818940148625)\n",
      "COMET INFO:     metrics/mAP_0.5:0.95 [3] : (0.0063224144156709475, 0.04390545364739387)\n",
      "COMET INFO:     metrics/precision [3]    : (0.39164587322841493, 0.7637847181089455)\n",
      "COMET INFO:     metrics/recall [3]       : (0.08613549147004067, 0.2049469964664311)\n",
      "COMET INFO:     train/box_loss [3]       : (0.06513966619968414, 0.10548117756843567)\n",
      "COMET INFO:     train/cls_loss [3]       : (0.020902883261442184, 0.03395918756723404)\n",
      "COMET INFO:     train/obj_loss [3]       : (0.04623137041926384, 0.062126316130161285)\n",
      "COMET INFO:     val/box_loss [3]         : (0.060278505086898804, 0.08179974555969238)\n",
      "COMET INFO:     val/cls_loss [3]         : (0.01696421019732952, 0.0205827709287405)\n",
      "COMET INFO:     val/obj_loss [3]         : (0.025723250582814217, 0.03928018733859062)\n",
      "COMET INFO:     x/lr0 [3]                : (0.004148837209302328, 0.07069767441860465)\n",
      "COMET INFO:     x/lr1 [3]                : (0.0032558139534883722, 0.004414728682170543)\n",
      "COMET INFO:     x/lr2 [3]                : (0.0032558139534883722, 0.004414728682170543)\n",
      "COMET INFO:   Others:\n",
      "COMET INFO:     Created from                : YOLOv5\n",
      "COMET INFO:     comet_log_batch_metrics     : False\n",
      "COMET INFO:     comet_log_confusion_matrix  : True\n",
      "COMET INFO:     comet_log_per_class_metrics : False\n",
      "COMET INFO:     comet_max_image_uploads     : 100\n",
      "COMET INFO:     comet_mode                  : online\n",
      "COMET INFO:     comet_model_name            : yolov5\n",
      "COMET INFO:     offline_experiment          : True\n",
      "COMET INFO:   Parameters:\n",
      "COMET INFO:     anchor_t           : 4.0\n",
      "COMET INFO:     artifact_alias     : latest\n",
      "COMET INFO:     batch_size         : 16\n",
      "COMET INFO:     bbox_interval      : -1\n",
      "COMET INFO:     box                : 0.05\n",
      "COMET INFO:     bucket             : \n",
      "COMET INFO:     cfg                : \n",
      "COMET INFO:     cls                : 0.01875\n",
      "COMET INFO:     cls_pw             : 1.0\n",
      "COMET INFO:     copy_paste         : 0.0\n",
      "COMET INFO:     cos_lr             : False\n",
      "COMET INFO:     degrees            : 0.0\n",
      "COMET INFO:     device             : \n",
      "COMET INFO:     entity             : 1\n",
      "COMET INFO:     evolve             : 1\n",
      "COMET INFO:     exist_ok           : False\n",
      "COMET INFO:     fl_gamma           : 0.0\n",
      "COMET INFO:     fliplr             : 0.5\n",
      "COMET INFO:     flipud             : 0.0\n",
      "COMET INFO:     freeze             : [0]\n",
      "COMET INFO:     hsv_h              : 0.015\n",
      "COMET INFO:     hsv_s              : 0.7\n",
      "COMET INFO:     hsv_v              : 0.4\n",
      "COMET INFO:     hyp                : {\"anchor_t\": 4.0, \"box\": 0.05, \"cls\": 0.5, \"cls_pw\": 1.0, \"copy_paste\": 0.0, \"degrees\": 0.0, \"fl_gamma\": 0.0, \"fliplr\": 0.5, \"flipud\": 0.0, \"hsv_h\": 0.015, \"hsv_s\": 0.7, \"hsv_v\": 0.4, \"iou_t\": 0.2, \"lr0\": 0.01, \"lrf\": 0.01, \"mixup\": 0.0, \"momentum\": 0.937, \"mosaic\": 1.0, \"obj\": 1.0, \"obj_pw\": 1.0, \"perspective\": 0.0, \"scale\": 0.5, \"shear\": 0.0, \"translate\": 0.1, \"warmup_bias_lr\": 0.1, \"warmup_epochs\": 3.0, \"warmup_momentum\": 0.8, \"weight_decay\": 0.0005}\n",
      "COMET INFO:     image_weights      : False\n",
      "COMET INFO:     imgsz              : 640\n",
      "COMET INFO:     iou_t              : 0.2\n",
      "COMET INFO:     label_smoothing    : 0.0\n",
      "COMET INFO:     local_rank         : -1\n",
      "COMET INFO:     lr0                : 0.01\n",
      "COMET INFO:     lrf                : 0.01\n",
      "COMET INFO:     mixup              : 0.0\n",
      "COMET INFO:     momentum           : 0.937\n",
      "COMET INFO:     mosaic             : 1.0\n",
      "COMET INFO:     multi_scale        : False\n",
      "COMET INFO:     name               : exp\n",
      "COMET INFO:     noautoanchor       : False\n",
      "COMET INFO:     noplots            : False\n",
      "COMET INFO:     nosave             : False\n",
      "COMET INFO:     noval              : False\n",
      "COMET INFO:     obj                : 1.0\n",
      "COMET INFO:     obj_pw             : 1.0\n",
      "COMET INFO:     optimizer          : SGD\n",
      "COMET INFO:     patience           : 100\n",
      "COMET INFO:     perspective        : 0.0\n",
      "COMET INFO:     project            : runs/train\n",
      "COMET INFO:     quad               : False\n",
      "COMET INFO:     rect               : False\n",
      "COMET INFO:     resume             : False\n",
      "COMET INFO:     save_dir           : runs/train/exp2\n",
      "COMET INFO:     save_period        : -1\n",
      "COMET INFO:     scale              : 0.5\n",
      "COMET INFO:     seed               : 0\n",
      "COMET INFO:     shear              : 0.0\n",
      "COMET INFO:     single_cls         : False\n",
      "COMET INFO:     sync_bn            : False\n",
      "COMET INFO:     translate          : 0.1\n",
      "COMET INFO:     upload_dataset     : False\n",
      "COMET INFO:     val_conf_threshold : 0.001\n",
      "COMET INFO:     val_iou_threshold  : 0.6\n",
      "COMET INFO:     warmup_bias_lr     : 0.1\n",
      "COMET INFO:     warmup_epochs      : 3.0\n",
      "COMET INFO:     warmup_momentum    : 0.8\n",
      "COMET INFO:     weight_decay       : 0.0005\n",
      "COMET INFO:     workers            : 8\n",
      "COMET INFO:   Uploads:\n",
      "COMET INFO:     asset               : 13 (1.53 MB)\n",
      "COMET INFO:     confusion-matrix    : 1\n",
      "COMET INFO:     environment details : 1\n",
      "COMET INFO:     images              : 15\n",
      "COMET INFO:     installed packages  : 1\n",
      "COMET INFO:     os packages         : 1\n",
      "COMET INFO: ----------------------------------\n",
      "COMET WARNING: Experiment Name is generated at upload time for Offline Experiments unless set explicitly with Experiment.set_name\n",
      "COMET WARNING: Comet has disabled auto-logging functionality as it has been imported after the following ML modules: torch, fastai, tensorboard, sklearn. Metrics and hyperparameters can still be logged using Experiment.log_metrics() and Experiment.log_parameters()\n",
      "COMET INFO: Still saving offline stats to messages file before program termination (may take up to 120 seconds)\n",
      "COMET INFO: Starting saving the offline archive\n",
      "COMET INFO: To upload this offline experiment, run:\n",
      "    comet upload /content/yolov5/.cometml-runs/39863b8b3f06409b9494e5e5cffb888d.zip\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 640 --batch 16 --epochs 3 --data face_mask_detection.yaml --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmyI5AkJLmgl"
   },
   "source": [
    "##**Inference**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5175,
     "status": "ok",
     "timestamp": 1675979998725,
     "user": {
      "displayName": "Mehdi Hamzeloee",
      "userId": "13945648688805362432"
     },
     "user_tz": -210
    },
    "id": "qu52UpE5Ll0o",
    "outputId": "bfdab9fe-f398-4eb9-e441-3ff91963dbf4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['/content/yolov5/runs/train/exp2/weights/best.pt'], source=/content/images/test/maksssksksss140.png, data=data/coco128.yaml, imgsz=[640, 640], conf_thres=0.25, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs/detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "YOLOv5 🚀 v7.0-89-g35d6d9f Python-3.8.10 torch-1.13.1+cu116 CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7018216 parameters, 0 gradients, 15.8 GFLOPs\n",
      "image 1/1 /content/images/test/maksssksksss140.png: 448x640 4 with_masks, 256.9ms\n",
      "Speed: 1.9ms pre-process, 256.9ms inference, 1.2ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/detect/exp2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python /content/yolov5/detect.py --weights /content/yolov5/runs/train/exp2/weights/best.pt --img 640 --conf 0.25 --source /content/images/test/maksssksksss140.png"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMfxRWKolaJKTo10yYA4dnl",
   "collapsed_sections": [
    "gb7RlfW-JWoU",
    "1d7isOkPJiHh",
    "RabIljo8P4-s",
    "LmjtlcF9udGW",
    "VZTIgDmFpg8g"
   ],
   "mount_file_id": "1lw3tZl4B7CfKFYPYgucnYNCAz0x3Btsc",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
